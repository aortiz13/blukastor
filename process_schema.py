import json
import re
import os

def extract_json(filepath):
    """Reads a file and extracts the JSON array from within the untrusted-data blocks."""
    with open(filepath, 'r') as f:
        content = f.read()
    
    # Try to find the JSON array directly
    # It usually starts with [{" and ends with }]
    match = re.search(r'(\[\s*\{.*\}\s*\])', content, re.DOTALL)
    
    json_str = ""
    if match:
        json_str = match.group(1)
    else:
        # Fallback: try to find just [] if empty or other format
        match = re.search(r'(\[\s*\])', content, re.DOTALL)
        if match:
            json_str = match.group(1)
    
    if not json_str:
        # If regex failed, maybe try the whole content if it looks like json
        if content.strip().startswith('[') and content.strip().endswith(']'):
            json_str = content
    
    if json_str:
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"JSON decode error in {filepath}: {e}")
            print(f"Snippet: {json_str[:100]}...")
            
            # Fallback 1: aggressive unescape
            try:
                cleaned = json_str.replace('\\"', '"')
                return json.loads(cleaned)
            except:
                pass
                
            # Fallback 2: try to treat as python literal
            try:
                import ast
                # Python uses True/False/None. JSON uses true/false/null.
                # We can try to replace them.
                py_str = json_str.replace('true', 'True').replace('false', 'False').replace('null', 'None')
                return ast.literal_eval(py_str)
            except:
                print("Fallback parsing also failed.")
                return []
    
    print(f"Failed to extract JSON from {filepath}")
    return []

def generate_schema(functions_path, triggers_path, columns_path, constraints_path, output_file):
    functions = extract_json(functions_path)
    triggers = extract_json(triggers_path)
    columns = extract_json(columns_path)
    constraints = extract_json(constraints_path)

    print(f"Loaded {len(functions)} functions")
    print(f"Loaded {len(triggers)} triggers")
    print(f"Loaded {len(columns)} columns")
    print(f"Loaded {len(constraints)} constraints")

    with open(output_file, 'w') as f:
        f.write("-- Schema Export for project jgnvjqfvlgljmwwazmea\n")
        f.write("-- Generated by Antigravity Agent via Supabase MCP\n\n")

        # 1. Schemas (ensure they exist)
        schemas = set()
        for item in columns:
            if item.get('table_schema'):
                schemas.add(item['table_schema'])
        for schema in schemas:
            if schema != 'public':
                f.write(f"CREATE SCHEMA IF NOT EXISTS {schema};\n")
        f.write("\n")

        # 2. Tables (Base definitions)
        # Group columns by table
        tables = {}
        for col in columns:
            schema = col.get('table_schema')
            table = col.get('table_name')
            if not schema or not table: continue
            
            key = (schema, table)
            if key not in tables:
                tables[key] = []
            tables[key].append(col)
        
        # Group PKs
        pks = {} # (schema, table) -> [col_names]
        for c in constraints:
            if c.get('constraint_type') == 'PRIMARY KEY':
                key = (c.get('table_schema'), c.get('table_name'))
                if key not in pks:
                    pks[key] = []
                pks[key].append(c['column_name'])

        for (schema, table), cols in tables.items():
            f.write(f"CREATE TABLE IF NOT EXISTS {schema}.{table} (\n")
            col_defs = []
            
            # Sort cols by ordinal position if possible, but they came ordered from query
            for col in cols:
                dtype = col['data_type']
                nullable = "NULL" if col['is_nullable'] == 'YES' else "NOT NULL"
                default = f"DEFAULT {col['column_default']}" if col.get('column_default') else ""
                col_defs.append(f"    {col['column_name']} {dtype} {nullable} {default}")
            
            if (schema, table) in pks:
                # Deduplicate PK cols just in case
                pk_cols_list = [] 
                seen = set()
                for x in pks[(schema, table)]:
                    if x not in seen:
                        pk_cols_list.append(x)
                        seen.add(x)
                pk_cols = ", ".join(pk_cols_list)
                col_defs.append(f"    CONSTRAINT {table}_pkey PRIMARY KEY ({pk_cols})")

            f.write(",\n".join(col_defs))
            f.write("\n);\n\n")

        # 3. Foreign Keys (ALTER TABLE)
        f.write("-- Foreign Keys\n")
        
        # Group constraints by name to handle composites
        constraint_map = {}
        for c in constraints:
            c_name = c.get('constraint_name')
            if not c_name: continue
            
            if c_name not in constraint_map:
                constraint_map[c_name] = {
                    'type': c.get('constraint_type'),
                    'schema': c.get('table_schema'),
                    'table': c.get('table_name'),
                    'columns': [],
                    'foreign_schema': c.get('foreign_table_schema'),
                    'foreign_table': c.get('foreign_table_name'),
                    'foreign_columns': []
                }
            # Append cols
            if c.get('column_name'):
                constraint_map[c_name]['columns'].append(c['column_name'])
            if c.get('foreign_column_name'):
                constraint_map[c_name]['foreign_columns'].append(c['foreign_column_name'])

        for c_name, c_top in constraint_map.items():
            if c_top['type'] == 'FOREIGN KEY':
                # Skip if foreign table info is missing
                if not c_top['foreign_schema'] or not c_top['foreign_table']:
                    print(f"Skipping incomplete FK {c_name}: {c_top}")
                    continue
                    
                cols = ", ".join(c_top['columns'])
                f_cols = ", ".join(c_top['foreign_columns'])
                f.write(f"ALTER TABLE {c_top['schema']}.{c_top['table']} ADD CONSTRAINT {c_name} ")
                f.write(f"FOREIGN KEY ({cols}) REFERENCES {c_top['foreign_schema']}.{c_top['foreign_table']} ({f_cols});\n")
        f.write("\n")

        # 4. Functions
        f.write("-- Functions\n")
        # Functions output is a list of dicts: schema_name, function_name, arguments, definition
        for func in functions:
            # The definition usually includes 'CREATE FUNCTION ...'
            # But sometimes it might be truncated or formatted cleanly.
            # The 'definition' field from our query `pg_get_functiondef` (presumably) contains the full CREATE statement.
            # Let's write it out.
            f.write(func['definition'])
            f.write(";\n\n")

        # 5. Triggers
        f.write("-- Triggers\n")
        for trig in triggers:
            # Definition has 'CREATE TRIGGER ...'
            f.write(trig['definition'])
            f.write(";\n\n")

if __name__ == "__main__":
    # Paths based on the agent's context
    base_path = "/Users/adrianortiz/.gemini/antigravity/brain/f991e276-e051-4461-a202-16c290ac3ed5/.system_generated/steps"
    
    # Adjust step IDs based on your history
    # Step 191: Functions
    # Step 206: Triggers
    # Step 217: Columns
    # Step 221: Constraints
    
    func_path = f"{base_path}/191/output.txt"
    trig_path = f"{base_path}/206/output.txt"
    col_path = f"{base_path}/217/output.txt"
    cons_path = f"{base_path}/221/output.txt"
    
    output_path = "/Users/adrianortiz/Desktop/Blukastor/database/schema_dump_v1.sql"
    
    generate_schema(func_path, trig_path, col_path, cons_path, output_path)
    print(f"Schema dump generated at {output_path}")
